# Change: 添加解码器微调模式支持

## Why

当前系统仅支持对编码器模型（BERT、RoBERTa、SentenceBERT等）进行微调，这些模型擅长理解和提取文本特征，但不适合生成任务。为了扩展系统的能力，需要支持对解码器模型（Qwen、LLaMA等生成模型）进行微调，以利用这些模型强大的语言理解和生成能力来提升图节点分类和零样本迁移的性能。

解码器模型在预训练时学习了更丰富的语言知识，可能在理解复杂的数据集描述和标签文本时表现更好，从而提升跨数据集的零样本迁移效果。

## What Changes

- **新增解码器模型支持**: 添加对 Qwen、LLaMA 等因果语言模型（Causal LM）的支持
- **扩展 Text_Lora 类**: 支持解码器架构的 LoRA 微调配置
- **修改特征提取逻辑**: 解码器模型使用最后一个 token 的隐藏状态而非第一个 token（[CLS]）
- **更新模型配置**: 针对不同解码器模型配置正确的 target_modules 和 tokenizer 设置
- **增强命令行参数**: 添加 `--model_type` 参数用于区分编码器和解码器模式
- **添加新的文本编码器选项**: 支持 'qwen', 'llama3', 'mistral' 等主流开源生成模型

## Impact

### 受影响的规范
- `text-model` (NEW) - 文本模型架构和微调策略的规范

### 受影响的代码
- `code/st_model.py` - 核心修改，添加解码器模型支持
- `code/main_TextBP_benchmark.py` - 更新参数解析和模型初始化逻辑
- `requirements.txt` - 可能需要更新依赖版本以支持新模型

### 兼容性
- **向后兼容**: 现有的编码器微调模式将继续工作
- **配置变更**: 需要在命令行中明确指定解码器模型类型
- **性能影响**: 解码器模型通常更大，推理速度可能较慢，需要更多 GPU 内存

### 风险
- 解码器模型的内存占用可能比编码器大 2-4 倍
- 需要验证不同解码器模型在图学习任务上的实际效果
- 可能需要调整学习率、LoRA 秩等超参数
